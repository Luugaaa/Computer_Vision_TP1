{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00c4d9",
   "metadata": {},
   "source": [
    "# Histogrammes des Orientations du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d8915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in ./.venv/lib/python3.11/site-packages (4.13.0.90)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.8)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting scipy>=1.11.4 (from scikit-image)\n",
      "  Using cached scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting networkx>=3.0 (from scikit-image)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2026.1.14-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading scikit_image-0.26.0-cp311-cp311-macosx_11_0_arm64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl (20.1 MB)\n",
      "Downloading tifffile-2026.1.14-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: tifffile, scipy, networkx, lazy-loader, imageio, scikit-image\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [scikit-image][0m [scikit-image]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imageio-2.37.2 lazy-loader-0.4 networkx-3.6.1 scikit-image-0.26.0 scipy-1.17.0 tifffile-2026.1.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python-headless numpy matplotlib tqdm scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a77f9",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653ba3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31669b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "data_root_path = \"data/UCF-101\"\n",
    "\n",
    "CLASS_NAMES = sorted(os.listdir(data_root_path))\n",
    "if '.DS_Store' in CLASS_NAMES:\n",
    "    CLASS_NAMES.remove('.DS_Store')\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cd05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to extract frames from a video file\n",
    "def extract_all_frames_from_video(video_path, frame_rate=3):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return frames\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(fps / frame_rate)\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "## Function to extract frame data for all videos in the dataset\n",
    "def extract_dataset_all_frames(data_root_path, frame_rate=100):\n",
    "    dataset_frames = {}\n",
    "    for class_name in tqdm(CLASS_NAMES, desc=\"Processing classes\"):\n",
    "        class_path = os.path.join(data_root_path, class_name)\n",
    "        video_files = [f for f in os.listdir(class_path) if f.endswith('.avi')]\n",
    "        dataset_frames[class_name] = []\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing videos in {class_name}\", leave=False):\n",
    "            video_path = os.path.join(class_path, video_file)\n",
    "            frames = extract_frames_from_video(video_path, frame_rate)\n",
    "            dataset_frames[class_name].extend(frames)\n",
    "    return dataset_frames\n",
    "\n",
    "def split_dataset(dataset, reference_ratio=0.75):\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    for class_name, items in dataset.items():\n",
    "        random.shuffle(items)\n",
    "        split_index = int(len(items) * reference_ratio)\n",
    "        train_set[class_name] = items[:split_index]\n",
    "        test_set[class_name] = items[split_index:]\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing video stats for classes: 100%|██████████| 5/5 [00:16<00:00,  3.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Biking': {'num_videos': 134,\n",
       "  'total_frames': 6109,\n",
       "  'avg_frames_per_video': 45.58955223880597,\n",
       "  'min_frames_per_video': 21,\n",
       "  'max_frames_per_video': 120,\n",
       "  'avg_frame_size': array([240., 320.,   3.]),\n",
       "  'min_frame_size': array([240, 320,   3]),\n",
       "  'max_frame_size': array([240, 320,   3]),\n",
       "  'avg_video_length_sec': np.float64(9.117910447761194),\n",
       "  'min_video_length_sec': 4.2,\n",
       "  'max_video_length_sec': 24.0},\n",
       " 'HandstandPushups': {'num_videos': 128,\n",
       "  'total_frames': 3007,\n",
       "  'avg_frames_per_video': 23.4921875,\n",
       "  'min_frames_per_video': 10,\n",
       "  'max_frames_per_video': 51,\n",
       "  'avg_frame_size': array([240., 320.,   3.]),\n",
       "  'min_frame_size': array([240, 320,   3]),\n",
       "  'max_frame_size': array([240, 320,   3]),\n",
       "  'avg_video_length_sec': np.float64(4.6984375),\n",
       "  'min_video_length_sec': 2.0,\n",
       "  'max_video_length_sec': 10.2},\n",
       " 'PlayingCello': {'num_videos': 164,\n",
       "  'total_frames': 7597,\n",
       "  'avg_frames_per_video': 46.323170731707314,\n",
       "  'min_frames_per_video': 21,\n",
       "  'max_frames_per_video': 83,\n",
       "  'avg_frame_size': array([240., 320.,   3.]),\n",
       "  'min_frame_size': array([240, 320,   3]),\n",
       "  'max_frame_size': array([240, 320,   3]),\n",
       "  'avg_video_length_sec': np.float64(9.264634146341463),\n",
       "  'min_video_length_sec': 4.2,\n",
       "  'max_video_length_sec': 16.6},\n",
       " 'PlayingViolin': {'num_videos': 100,\n",
       "  'total_frames': 3721,\n",
       "  'avg_frames_per_video': 37.21,\n",
       "  'min_frames_per_video': 29,\n",
       "  'max_frames_per_video': 102,\n",
       "  'avg_frame_size': array([240., 320.,   3.]),\n",
       "  'min_frame_size': array([240, 320,   3]),\n",
       "  'max_frame_size': array([240, 320,   3]),\n",
       "  'avg_video_length_sec': np.float64(7.441999999999999),\n",
       "  'min_video_length_sec': 5.8,\n",
       "  'max_video_length_sec': 20.4},\n",
       " 'PushUps': {'num_videos': 102,\n",
       "  'total_frames': 1721,\n",
       "  'avg_frames_per_video': 16.872549019607842,\n",
       "  'min_frames_per_video': 6,\n",
       "  'max_frames_per_video': 34,\n",
       "  'avg_frame_size': array([240., 320.,   3.]),\n",
       "  'min_frame_size': array([240, 320,   3]),\n",
       "  'max_frame_size': array([240, 320,   3]),\n",
       "  'avg_video_length_sec': np.float64(3.3745098039215686),\n",
       "  'min_video_length_sec': 1.2,\n",
       "  'max_video_length_sec': 6.8}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the videos in the data root, we compute some stats about the videos: number of frames, video length, frame size, etc\n",
    "def compute_video_stats(data_root_path):\n",
    "    stats = {}\n",
    "    for class_name in tqdm(CLASS_NAMES, desc=\"Computing video stats for classes\"):\n",
    "        class_path = os.path.join(data_root_path, class_name)\n",
    "        video_files = [f for f in os.listdir(class_path) if f.endswith('.avi')]\n",
    "        num_videos = len(video_files)\n",
    "        total_frames = 0\n",
    "        frames = []\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing videos in {class_name}\", leave=False):\n",
    "            video_path = os.path.join(class_path, video_file)\n",
    "            video_frames = extract_frames_from_video(video_path, frame_rate=5)\n",
    "            total_frames += len(video_frames)\n",
    "            frames.append(video_frames)\n",
    "        stats[class_name] = {\n",
    "            \"num_videos\": num_videos,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"avg_frames_per_video\": total_frames / num_videos,\n",
    "            \"min_frames_per_video\": min([len(frames) for frames in frames]),\n",
    "            \"max_frames_per_video\": max([len(frames) for frames in frames]),\n",
    "            \"avg_frame_size\": np.mean([frame.shape for frames in frames for frame in frames], axis=0),\n",
    "            \"min_frame_size\": np.min([frame.shape for frames in frames for frame in frames], axis=0),\n",
    "            \"max_frame_size\": np.max([frame.shape for frames in frames for frame in frames], axis=0),\n",
    "            \"avg_video_length_sec\": np.mean([len(frames)/5 for frames in frames]),\n",
    "            \"min_video_length_sec\": min([len(frames)/5 for frames in frames]),\n",
    "            \"max_video_length_sec\": max([len(frames)/5 for frames in frames]),\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "compute_video_stats(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d34c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 5/5 [00:16<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "full_frames_dataset = extract_dataset_frames(data_root_path, frame_rate=1)\n",
    "reference_frame_dataset, test_frame_dataset = split_dataset(full_frames_dataset, reference_ratio=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846e48f",
   "metadata": {},
   "source": [
    "## HOG Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003e49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## HOG feature extraction \n",
    "def extract_hog_features_for_image(image):\n",
    "    fd, hog_image = hog(\n",
    "        image,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(16, 16),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=True,\n",
    "        channel_axis=-1,\n",
    "    )\n",
    "    return fd, hog_image\n",
    "\n",
    "## HOG Dataset feature extraction  \n",
    "def extract_hog_features_for_dataset(dataset_frames):\n",
    "    dataset_hog_features = {}\n",
    "    for class_name, frames in tqdm(dataset_frames.items(), desc=\"Extracting HOG features\"):\n",
    "        dataset_hog_features[class_name] = []\n",
    "        for frame in tqdm(frames, desc=f\"Processing frames in {class_name})\"):\n",
    "            fd, hog_image = extract_hog_features_for_image(frame)\n",
    "            dataset_hog_features[class_name].append((fd, hog_image))\n",
    "    return dataset_hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a1572e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames in Biking):  36%|███▌      | 1646/4581 [00:49<01:29, 32.95it/s]\n",
      "Extracting HOG features:   0%|          | 0/5 [00:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hog_reference_features_dataset = \u001b[43mextract_hog_features_for_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_frame_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m hog_test_features_dataset = extract_hog_features_for_dataset(test_frame_dataset)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mextract_hog_features_for_dataset\u001b[39m\u001b[34m(dataset_frames)\u001b[39m\n\u001b[32m     17\u001b[39m     dataset_hog_features[class_name] = []\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m tqdm(frames, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing frames in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         fd, hog_image = \u001b[43mextract_hog_features_for_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m         dataset_hog_features[class_name].append((fd, hog_image))\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_hog_features\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mextract_hog_features_for_image\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_hog_features_for_image\u001b[39m(image):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     fd, hog_image = \u001b[43mhog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixels_per_cell\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcells_per_block\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fd, hog_image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local/Poly/Computer Vision/TP1/.venv/lib/python3.11/site-packages/skimage/_shared/utils.py:598\u001b[39m, in \u001b[36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33monly a single channel axis is currently supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m channel_axis == (-\u001b[32m1\u001b[39m,) \u001b[38;5;129;01mor\u001b[39;00m channel_axis == -\u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arg_positions:\n\u001b[32m    601\u001b[39m     new_args = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local/Poly/Computer Vision/TP1/.venv/lib/python3.11/site-packages/skimage/feature/_hog.py:292\u001b[39m, in \u001b[36mhog\u001b[39m\u001b[34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[39m\n\u001b[32m    285\u001b[39m                 centre = \u001b[38;5;28mtuple\u001b[39m([r * c_row + c_row // \u001b[32m2\u001b[39m, c * c_col + c_col // \u001b[32m2\u001b[39m])\n\u001b[32m    286\u001b[39m                 rr, cc = draw.line(\n\u001b[32m    287\u001b[39m                     \u001b[38;5;28mint\u001b[39m(centre[\u001b[32m0\u001b[39m] - dc),\n\u001b[32m    288\u001b[39m                     \u001b[38;5;28mint\u001b[39m(centre[\u001b[32m1\u001b[39m] + dr),\n\u001b[32m    289\u001b[39m                     \u001b[38;5;28mint\u001b[39m(centre[\u001b[32m0\u001b[39m] + dc),\n\u001b[32m    290\u001b[39m                     \u001b[38;5;28mint\u001b[39m(centre[\u001b[32m1\u001b[39m] - dr),\n\u001b[32m    291\u001b[39m                 )\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m                 hog_image[rr, cc] += orientation_histogram[r, c, o]\n\u001b[32m    294\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03mThe fourth stage computes normalization, which takes local groups of\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03mcells and contrast normalizes their overall responses before passing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m \u001b[33;03mGradient (HOG) descriptors.\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m n_blocks_row = (n_cells_row - b_row) + \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "hog_reference_features_dataset = extract_hog_features_for_dataset(reference_frame_dataset)\n",
    "hog_test_features_dataset = extract_hog_features_for_dataset(test_frame_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3c1e4",
   "metadata": {},
   "source": [
    "## Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_hog_averages(hog_features_dataset):\n",
    "    class_hog_averages = {}\n",
    "    for class_name, features in hog_features_dataset.items():\n",
    "        feature_vectors = [fd for fd, hog_image in features]\n",
    "        average_fd = np.mean(feature_vectors, axis=0)\n",
    "        class_hog_averages[class_name] = average_fd\n",
    "    return class_hog_averages\n",
    "\n",
    "class_hog_averages = compute_class_hog_averages(hog_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_distance(fd1, fd2):\n",
    "    return np.linalg.norm(fd1 - fd2)\n",
    "\n",
    "def classify_frame_hog(fd, class_hog_averages):\n",
    "    min_distance = float('inf')\n",
    "    predicted_class = None\n",
    "    for class_name, average_fd in class_hog_averages.items():\n",
    "        distance = measure_distance(fd, average_fd)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            predicted_class = class_name\n",
    "    return predicted_class\n",
    "\n",
    "## Evaluation: cmopute preicison, recall and f1 and create confusion matrix\n",
    "def evaluate_classification(hog_test_features_dataset, class_hog_averages):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for class_name, features in hog_test_features_dataset.items():\n",
    "        for fd, hog_image in features:\n",
    "            predicted_class = classify_frame_hog(fd, class_hog_averages)\n",
    "            y_true.append(class_name)\n",
    "            y_pred.append(predicted_class)\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=CLASS_NAMES)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
